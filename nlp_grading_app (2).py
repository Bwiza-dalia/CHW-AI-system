# -*- coding: utf-8 -*-
"""nlp_grading_app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HSYGbGKQhpxT92Ka0SrPCQi0p8F27Jp2

Import Libraries and Download NLTK Resources
"""

# Import all required libraries
import numpy as np
import pandas as pd
import re
import string
from collections import Counter
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
import pickle
import json

import nltk
try:
    nltk.data.find('tokenizers/punkt_tab/')
    print("NLTK punkt_tab data already downloaded.")
except LookupError:
    print("Downloading NLTK punkt_tab data...")
    nltk.download('punkt_tab', quiet=True)
    print("NLTK punkt_tab data downloaded.")

from urllib.error import URLError

try:
    nltk.download('punkt')
except URLError as e:
    print("Network error while downloading NLTK data:", e)

# Download necessary NLTK resources
nltk.download('punkt', quiet=True)
nltk.download('stopwords', quiet=True)
print("NLTK data downloaded.")

"""Define the Grading Class - Initialization"""

# Main class for grading
class BilingualNLPGrader:
    def __init__(self):
        self.tfidf_vectorizer = TfidfVectorizer(
            max_features=5000,
            stop_words='english',
            ngram_range=(1, 2),
            lowercase=True
        )
        # Load a pretrained sentence transformer model for semantic similarity
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
        self.stemmer = PorterStemmer()
        self.model = RandomForestRegressor(n_estimators=100, random_state=42)
        self.scaler = StandardScaler()
        self.is_trained = False
        self.english_stopwords = set(stopwords.words('english'))
        self.kinyarwanda_stopwords = {
            'na', 'mu', 'ku', 'kuri', 'ni', 'cyangwa', 'nk', 'ko', 'kuko',
            'ariko', 'maze', 'naho', 'nyuma', 'mbere', 'hanyuma', 'kandi',
            'cyane', 'cane', 'buri', 'byose', 'byinshi'
        }

    def train(self, training_data):
        if not training_data:
            raise ValueError("Training data cannot be empty.")
        all_texts = []
        for item in training_data:
            ref_text = self.preprocess_text(item['reference_answer'], item.get('language', 'en'))
            stu_text = self.preprocess_text(item['student_answer'], item.get('language', 'en'))
            all_texts.extend([ref_text, stu_text])
        self.tfidf_vectorizer.fit(all_texts)
        self.tfidf_fitted = True
        X = []
        y = []
        for item in training_data:
            features = self.extract_features(
                item['reference_answer'],
                item['student_answer'],
                item.get('language', 'en')
            )
            X.append(list(features.values()))
            y.append(item['score'])
        X = self.scaler.fit_transform(np.array(X))
        y = np.array(y)
        self.model.fit(X, y)
        self.is_trained = True
        print("Model training complete.")

    def preprocess_text(self, text, language='en'):
        if not isinstance(text, str):
            return ""
        text = text.lower()
        text = re.sub(r'[^\w\s]', ' ', text)
        text = re.sub(r'\d+', '', text)
        text = re.sub(r'\s+', ' ', text).strip()
        tokens = word_tokenize(text)
        if language == 'en':
            tokens = [token for token in tokens if token not in self.english_stopwords]
            tokens = [self.stemmer.stem(token) for token in tokens]
        else:
            tokens = [token for token in tokens if token not in self.kinyarwanda_stopwords]
        return ' '.join(tokens)

    def extract_features(self, reference_text, student_text, language='en'):
        ref_processed = self.preprocess_text(reference_text, language)
        stu_processed = self.preprocess_text(student_text, language)
        features = {}
        features['length_ratio'] = len(student_text) / max(len(reference_text), 1)
        features['word_count_ratio'] = len(student_text.split()) / max(len(reference_text.split()), 1)
        try:
            if hasattr(self, 'tfidf_fitted') and self.tfidf_fitted:
                tfidf_matrix = self.tfidf_vectorizer.transform([ref_processed, stu_processed])
                cosine_sim = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]
                features['tfidf_similarity'] = cosine_sim
            else:
                features['tfidf_similarity'] = 0.0
        except:
            features['tfidf_similarity'] = 0.0
        ref_words = set(ref_processed.split())
        stu_words = set(stu_processed.split())
        features['jaccard_similarity'] = len(ref_words & stu_words) / max(len(ref_words | stu_words), 1)
        ref_terms = ref_processed.split()
        stu_terms = stu_processed.split()
        features['term_coverage'] = sum(1 for term in ref_terms if term in stu_terms) / len(ref_terms) if ref_terms else 0.0
        features['semantic_density'] = len(set(stu_terms)) / len(stu_terms) if stu_terms else 0.0
        health_keywords = {
            'en': ['health', 'disease', 'symptom', 'treatment', 'medicine', 'doctor',
                   'patient', 'infection', 'prevention', 'malaria', 'fever', 'nutrition',
                   'bed', 'net', 'mosquito', 'water', 'clinic', 'hospital'],
            'kin': ['ubuzima', 'indwara', 'ibimenyetso', 'ubuvuzi', 'umuganga',
                    'umurwayi', 'kwandura', 'kurinda', 'malariya', 'umuriro', 'intungamubiri',
                    'umuryango', 'ubunyangamugayo', 'amazi', 'ikigo', 'nderabuzima']
        }
        keywords = health_keywords.get(language, health_keywords['en'])
        keyword_count = sum(1 for word in stu_terms if word in keywords)
        features['keyword_density'] = keyword_count / max(len(stu_terms), 1)
        return features

    def grade(self, reference_answer, student_answer, language='en'):
        if not self.is_trained:
            raise ValueError("Model must be trained before grading.")
        features = self.extract_features(reference_answer, student_answer, language)
        feature_vector = np.array([list(features.values())])
        feature_scaled = self.scaler.transform(feature_vector)
        predicted_score = self.model.predict(feature_scaled)[0]
        final_score = max(1, min(5, round(predicted_score)))
        confidence = min(1.0, features['tfidf_similarity'] + features['jaccard_similarity'])
        feedback = self.generate_feedback(features, final_score, language)
        return {
            'score': final_score,
            'confidence': confidence,
            'features': features,
            'feedback': feedback
        }

    def generate_feedback(self, features, score, language='en'):
        feedback_templates = {
            'en': {
                5: "Excellent answer. You've covered the key points clearly.",
                4: "Good answer. Consider adding a few more details.",
                3: "Fair answer. Try to include more key terms and details.",
                2: "Your answer needs more explanation and examples.",
                1: "This answer is too brief or off-topic. Please review the material."
            },
            'kin': {
                5: "Igisubizo cyiza cyane. Wasobanuye ingingo zose neza.",
                4: "Igisubizo cyiza. Wagerageza kongeramo andi makuru.",
                3: "Hari ibyo wasobanura neza kurushaho.",
                2: "Ibisubizo bikwiye kuba birambuye kurushaho.",
                1: "Igisubizo ni kigufi cyangwa kidahuye n'ibikenewe."
            }
        }
        templates = feedback_templates.get(language, feedback_templates['en'])
        base_feedback = templates.get(score, templates[3])
        if features['term_coverage'] < 0.5:
            if language == 'en':
                base_feedback += " Try to mention more important terms from the reference."
            else:
                base_feedback += " Gerageza gushyiramo amagambo y'ingenzi yifashishijwe mu gisubizo cy'ukuri."
        return base_feedback

"""Text Preprocessing Function"""

def preprocess_text(self, text, language='en'):
        if not isinstance(text, str):
            return ""
        text = text.lower()
        text = re.sub(r'[^\w\s]', ' ', text)
        text = re.sub(r'\d+', '', text)
        text = re.sub(r'\s+', ' ', text).strip()
        tokens = word_tokenize(text)
        if language == 'en':
            tokens = [token for token in tokens if token not in self.english_stopwords]
            tokens = [self.stemmer.stem(token) for token in tokens]
        else:
            tokens = [token for token in tokens if token not in self.kinyarwanda_stopwords]
        return ' '.join(tokens)

"""Feature Extraction"""

def extract_features(self, reference_text, student_text, language='en'):
        ref_processed = self.preprocess_text(reference_text, language)
        stu_processed = self.preprocess_text(student_text, language)
        features = {}
        features['length_ratio'] = len(student_text) / max(len(reference_text), 1)
        features['word_count_ratio'] = len(student_text.split()) / max(len(reference_text.split()), 1)
        try:
            if hasattr(self, 'tfidf_fitted') and self.tfidf_fitted:
                tfidf_matrix = self.tfidf_vectorizer.transform([ref_processed, stu_processed])
                cosine_sim = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]
                features['tfidf_similarity'] = cosine_sim
            else:
                features['tfidf_similarity'] = 0.0
        except:
            features['tfidf_similarity'] = 0.0
        ref_words = set(ref_processed.split())
        stu_words = set(stu_processed.split())
        features['jaccard_similarity'] = len(ref_words & stu_words) / max(len(ref_words | stu_words), 1)
        ref_terms = ref_processed.split()
        stu_terms = stu_processed.split()
        features['term_coverage'] = sum(1 for term in ref_terms if term in stu_terms) / len(ref_terms) if ref_terms else 0.0
        features['semantic_density'] = len(set(stu_terms)) / len(stu_terms) if stu_terms else 0.0
        health_keywords = {
            'en': ['health', 'disease', 'symptom', 'treatment', 'medicine', 'doctor',
                   'patient', 'infection', 'prevention', 'malaria', 'fever', 'nutrition',
                   'bed', 'net', 'mosquito', 'water', 'clinic', 'hospital'],
            'kin': ['ubuzima', 'indwara', 'ibimenyetso', 'ubuvuzi', 'umuganga',
                    'umurwayi', 'kwandura', 'kurinda', 'malariya', 'umuriro', 'intungamubiri',
                    'umuryango', 'ubunyangamugayo', 'amazi', 'ikigo', 'nderabuzima']
        }
        keywords = health_keywords.get(language, health_keywords['en'])
        keyword_count = sum(1 for word in stu_terms if word in keywords)
        features['keyword_density'] = keyword_count / max(len(stu_terms), 1)
        return features

"""Train the Model"""

def create_sample_data():
    return [
        {
            'reference_answer': "The main malaria prevention methods include using insecticide-treated bed nets, indoor residual spraying, eliminating stagnant water, using repellents, and seeking early treatment.",
            'student_answer': "To prevent malaria, we should use treated bed nets every night, spray homes with insecticide, remove standing water, use repellent creams, and go to health center when we have fever.",
            'score': 5,
            'language': 'en'
        },
        {
            'reference_answer': "The main malaria prevention methods include using insecticide-treated bed nets, indoor residual spraying, eliminating stagnant water, using repellents, and seeking early treatment.",
            'student_answer': "Use bed nets and remove standing water to prevent malaria.",
            'score': 3,
            'language': 'en'
        },
        {
            'reference_answer': "Pregnant women should eat diverse foods including fruits, vegetables, proteins like beans and meat, dairy products, and take supplements like folic acid.",
            'student_answer': "Pregnant women need to eat fruits, vegetables, beans, meat, milk products and take folic acid for the baby's health.",
            'score': 5,
            'language': 'en'
        },
        {
            'reference_answer': "Signs of dehydration in children include dry mouth, sunken eyes, reduced urination, and lethargy.",
            'student_answer': "Children are thirsty and tired when dehydrated.",
            'score': 2,
            'language': 'en'
        }
    ]

"""Grading Function"""

def grade(self, reference_answer, student_answer, language='en'):
        if not self.is_trained:
            raise ValueError("Model must be trained before grading.")
        features = self.extract_features(reference_answer, student_answer, language)
        feature_vector = np.array([list(features.values())])
        feature_scaled = self.scaler.transform(feature_vector)
        predicted_score = self.model.predict(feature_scaled)[0]
        final_score = max(1, min(5, round(predicted_score)))
        confidence = min(1.0, features['tfidf_similarity'] + features['jaccard_similarity'])
        feedback = self.generate_feedback(features, final_score, language)
        return {
            'score': final_score,
            'confidence': confidence,
            'features': features,
            'feedback': feedback
        }

"""Grading Function"""

def generate_feedback(self, features, score, language='en'):
        feedback_templates = {
            'en': {
                5: "Excellent answer. You've covered the key points clearly.",
                4: "Good answer. Consider adding a few more details.",
                3: "Fair answer. Try to include more key terms and details.",
                2: "Your answer needs more explanation and examples.",
                1: "This answer is too brief or off-topic. Please review the material."
            },
            'kin': {
                5: "Igisubizo cyiza cyane. Wasobanuye ingingo zose neza.",
                4: "Igisubizo cyiza. Wagerageza kongeramo andi makuru.",
                3: "Hari ibyo wasobanura neza kurushaho.",
                2: "Ibisubizo bikwiye kuba birambuye kurushaho.",
                1: "Igisubizo ni kigufi cyangwa kidahuye n'ibikenewe."
            }
        }
        templates = feedback_templates.get(language, feedback_templates['en'])
        base_feedback = templates.get(score, templates[3])
        if features['term_coverage'] < 0.5:
            if language == 'en':
                base_feedback += " Try to mention more important terms from the reference."
            else:
                base_feedback += " Gerageza gushyiramo amagambo y'ingenzi yifashishijwe mu gisubizo cy'ukuri."
        return base_feedback

"""Create Sample Data"""

def create_sample_data():
    return [
        {
            'reference_answer': "The main malaria prevention methods include using insecticide-treated bed nets, indoor residual spraying, eliminating stagnant water, using repellents, and seeking early treatment.",
            'student_answer': "To prevent malaria, we should use treated bed nets every night, spray homes with insecticide, remove standing water, use repellent creams, and go to health center when we have fever.",
            'score': 5,
            'language': 'en'
        },
        {
            'reference_answer': "The main malaria prevention methods include using insecticide-treated bed nets, indoor residual spraying, eliminating stagnant water, using repellents, and seeking early treatment.",
            'student_answer': "Use bed nets and remove standing water to prevent malaria.",
            'score': 3,
            'language': 'en'
        },
        {
            'reference_answer': "Pregnant women should eat diverse foods including fruits, vegetables, proteins like beans and meat, dairy products, and take supplements like folic acid.",
            'student_answer': "Pregnant women need to eat fruits, vegetables, beans, meat, milk products and take folic acid for the baby's health.",
            'score': 5,
            'language': 'en'
        },
        {
            'reference_answer': "Signs of dehydration in children include dry mouth, sunken eyes, reduced urination, and lethargy.",
            'student_answer': "Children are thirsty and tired when dehydrated.",
            'score': 2,
            'language': 'en'
        }
    ]

"""Run the Demo"""

def run_demo():
    print("=" * 60)
    print("CHW NLP GRADING SYSTEM DEMO")
    print("=" * 60)

    # Download necessary NLTK resources, including punkt_tab
    import nltk
    try:
        nltk.data.find('tokenizers/punkt_tab/')
        print("NLTK punkt_tab data already downloaded.")
    except nltk.downloader.DownloadError:
        print("Downloading NLTK punkt_tab data...")
        nltk.download('punkt_tab', quiet=True)
        print("NLTK punkt_tab data downloaded.")
    except LookupError:
         print("Downloading NLTK punkt_tab data...")
         nltk.download('punkt_tab', quiet=True)
         print("NLTK punkt_tab data downloaded.")


    grader = BilingualNLPGrader()
    training_data = create_sample_data()
    grader.train(training_data)

    test_cases = [
        {
            'reference': "fever can kill a child.",
            'student': "i don't know",
            'language': 'en',
            'expected': "Should get high score - good paraphrasing"
        },
        {
            'reference': "Abagore batwite bagomba kurya ibiryo bitandukanye birimo imbuto n'imboga.",
            'student': "Abagore bafite inda bakwiye kurya imbto n'imbga zitandukanye.",
            'language': 'kin',
            'expected': "Should get good score - covers main points"
        }
    ]

    for i, test in enumerate(test_cases, 1):
        print(f"\nTest Case {i} ({test['language']}):")
        print(f"Reference: {test['reference']}")
        print(f"Student: {test['student']}")
        print(f"Expected: {test['expected']}")
        result = grader.grade(test['reference'], test['student'], test['language'])
        print(f"Score: {result['score']}/5")
        print(f"Confidence: {result['confidence']:.2f}")
        print(f"Feedback: {result['feedback']}")
        print("Feature Summary:")
        for k, v in result['features'].items():
            print(f"  {k}: {v:.3f}" if isinstance(v, float) else f"  {k}: {v}")

    print("\nGrader demo finished.")
    return grader

"""Run If Scripted"""

# Run demo and return a trained grader
trained_grader = run_demo()

"""to consider the context"""



from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity









"""Improving TF-IDF for Kinyarwanda"""

# download the kinyarwanda carpus

from google.colab import drive
# drive.mount('/content/drive')

import tarfile

# Path to your file in Google Drive
file_path = "/content/drive/My Drive/AI CHW/kin_community_2022.tar.gz"

# Extract it to a working directory
extract_path = "/content/kinyarwanda_corpus"

with tarfile.open(file_path, "r:gz") as tar:
    tar.extractall(path=extract_path)

print("Extraction complete.")

import os

extract_path = "/content/kinyarwanda_corpus"

for root, dirs, files in os.walk(extract_path):
    for name in files:
        print(os.path.join(root, name))

"""Load Kinyarwanda Sentences from Leipzig Corpus"""

sent_file = "/content/kinyarwanda_corpus/kin_community_2022/kin_community_2022-sentences.txt"

kinyarwanda_sentences = []
with open(sent_file, 'r', encoding='utf-8') as f:
    for line in f:
        parts = line.strip().split('\t')
        if len(parts) == 2:
            kinyarwanda_sentences.append(parts[1])  # sentence text is in the second column

print(f"Loaded {len(kinyarwanda_sentences)} Kinyarwanda sentences.")
print("Example:", kinyarwanda_sentences[0])

"""Preprocess: tokenize, lowercase"""

from nltk.tokenize import word_tokenize

def preprocess(text):
    return " ".join(word_tokenize(text.lower()))

processed_sentences = [preprocess(sent) for sent in kinyarwanda_sentences]

"""Compute TF-IDF matrix"""

from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform(processed_sentences)

"""Define a query and compute similarity"""

query = "Bwiza"  # you can replace this with any Kinyarwanda query
query_vec = vectorizer.transform([preprocess(query)])

# Compute cosine similarity
similarities = cosine_similarity(query_vec, tfidf_matrix).flatten()

# Get top 5 most similar sentence indices
top_indices = similarities.argsort()[-5:][::-1]

# Display top 5 results
print("Top similar sentences:")
for i in top_indices:
    print(f"{similarities[i]:.4f} -> {kinyarwanda_sentences[i]}")

"""Preprocessing Improvements"""

from sklearn.feature_extraction.text import TfidfVectorizer
import string

def preprocess_kinyarwanda(text):
    # Remove punctuation and lowercase
    text = text.translate(str.maketrans('', '', string.punctuation)).lower()
    return text.split() # Return a list of tokens

# TF-IDF with unigrams and bigrams
vectorizer = TfidfVectorizer(
    tokenizer=preprocess_kinyarwanda, # Use the named function here
    ngram_range=(1, 2),
    min_df=2,   # Remove very rare words
    max_df=0.95, # Remove very frequent non-informative words
)

"""TF-IDF + Dimensionality Reduction"""

from sklearn.decomposition import TruncatedSVD
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import Normalizer

svd = TruncatedSVD(n_components=100)
normalizer = Normalizer(copy=False)
lsa_pipeline = make_pipeline(vectorizer, svd, normalizer)

X_lsa = lsa_pipeline.fit_transform(kinyarwanda_sentences)

"""Wrap Into a Reusable Function"""

def search_sentences(query, top_n=5):
    expanded_query = expand_query(query)
    query_vec = vectorizer.transform([preprocess(expanded_query)])
    similarities = cosine_similarity(query_vec, tfidf_matrix).flatten()
    top_indices = similarities.argsort()[-top_n:][::-1]

    print(f"Query: {query}\n")
    for i in top_indices:
        print(f"{similarities[i]:.4f} -> {kinyarwanda_sentences[i]}\n")

"""Kinyarwanda Stopwords List"""

kinyarwanda_stopwords = [
    "ni", "ya", "na", "mu", "ku", "bya", "yagize", "bari", "uyu", "ibyo",
    "iyo", "kugira", "ubwo", "ubwo", "ibyo", "kandi", "ariko", "cyangwa",
    "nubwo", "ubwo", "icyo", "byose", "ubwo", "ubwo", "ntabwo", "ubwo",
    "yari", "iyo", "uko", "kuba", "ubwo"
]

"""Preprocessing Function"""

import re

def preprocess_text(text, stopwords):
    text = text.lower()
    text = re.sub(r'[^\w\s]', '', text)
    words = text.split()
    filtered_words = [word for word in words if word not in stopwords]
    return ' '.join(filtered_words)

"""TF-IDF with Unigrams + Bigrams"""

from sklearn.feature_extraction.text import TfidfVectorizer

def build_tfidf_model(sentences, stopwords):
    preprocessed = [preprocess_text(s, stopwords) for s in sentences]
    vectorizer = TfidfVectorizer(ngram_range=(1,2))
    tfidf_matrix = vectorizer.fit_transform(preprocessed)
    return vectorizer, tfidf_matrix

"""Similarity Search Function"""

from sklearn.metrics.pairwise import cosine_similarity

def find_similar_sentences(query, sentences, vectorizer, tfidf_matrix, stopwords, top_n=5):
    query_processed = preprocess_text(query, stopwords)
    query_vec = vectorizer.transform([query_processed])
    cosine_similarities = cosine_similarity(query_vec, tfidf_matrix).flatten()
    top_indices = cosine_similarities.argsort()[::-1][:top_n]
    return [(sentences[i], cosine_similarities[i]) for i in top_indices]

"""treamlit App Interface"""

# Save this as app.py
import streamlit as st

# Load everything
st.title("Kinyarwanda NLP Grading Tool")

query = st.text_area("Enter student response:")
if st.button("Find Similar Sentences"):
    similar = find_similar_sentences(query, sentences, vectorizer, tfidf_matrix, kinyarwanda_stopwords)
    for sent, score in similar:
        st.write(f"**Score**: {score:.4f}")
        st.write(sent)
        st.markdown("---")

"""Load Sentences From Your Corpus"""

with open('/content/kinyarwanda_corpus/kin_community_2022/kin_community_2022-sentences.txt') as f:
    sentences = [line.strip() for line in f if line.strip()]

vectorizer, tfidf_matrix = build_tfidf_model(sentences, kinyarwanda_stopwords)